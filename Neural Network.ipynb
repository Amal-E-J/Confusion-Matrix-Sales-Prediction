{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "966c752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:06\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.0 in /opt/anaconda/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda/lib/python3.9/site-packages (from tensorflow) (63.4.1)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: keras<2.12,>=2.11.0 in /home/amal/.local/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl (21.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /opt/anaconda/lib/python3.9/site-packages (from tensorflow) (1.21.5)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: packaging in /opt/anaconda/lib/python3.9/site-packages (from tensorflow) (21.3)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda/lib/python3.9/site-packages (from tensorflow) (4.3.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/anaconda/lib/python3.9/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/anaconda/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.2/177.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /opt/anaconda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda/lib/python3.9/site-packages (from packaging->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tensorboard-plugin-wit, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "\u001b[33m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/home/amal/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.22.1\n",
      "    Uninstalling protobuf-4.22.1:\n",
      "      Successfully uninstalled protobuf-4.22.1\n",
      "\u001b[33m  WARNING: The script google-oauthlib-tool is installed in '/home/amal/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33m  WARNING: The script tensorboard is installed in '/home/amal/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/amal/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 flatbuffers-23.3.3 gast-0.4.0 google-auth-2.16.2 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.3 libclang-15.0.6.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63834b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1638/1638 [==============================] - 3s 1ms/step - loss: 274829152.0000 - val_loss: 51838804.0000\n",
      "Epoch 2/10\n",
      "1638/1638 [==============================] - 2s 1ms/step - loss: 23299154.0000 - val_loss: 19727076.0000\n",
      "Epoch 3/10\n",
      "1638/1638 [==============================] - 2s 1ms/step - loss: 19280704.0000 - val_loss: 19655500.0000\n",
      "Epoch 4/10\n",
      "1638/1638 [==============================] - 2s 1ms/step - loss: 19245854.0000 - val_loss: 19634374.0000\n",
      "Epoch 5/10\n",
      "1638/1638 [==============================] - 2s 1ms/step - loss: 19234498.0000 - val_loss: 19621084.0000\n",
      "Epoch 6/10\n",
      "1638/1638 [==============================] - 2s 1ms/step - loss: 19220558.0000 - val_loss: 19605580.0000\n",
      "Epoch 7/10\n",
      "1638/1638 [==============================] - 2s 1ms/step - loss: 19203682.0000 - val_loss: 19590198.0000\n",
      "Epoch 8/10\n",
      "1638/1638 [==============================] - 2s 1ms/step - loss: 19187294.0000 - val_loss: 19571294.0000\n",
      "Epoch 9/10\n",
      "1638/1638 [==============================] - 2s 1ms/step - loss: 19173460.0000 - val_loss: 19573862.0000\n",
      "Epoch 10/10\n",
      "1638/1638 [==============================] - 2s 1ms/step - loss: 19156814.0000 - val_loss: 19543534.0000\n",
      "1638/1638 [==============================] - 1s 734us/step\n",
      "Validation set RMSE: 4420.807233650857\n",
      "3509/3509 [==============================] - 3s 735us/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# read train dataset\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# split train dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "# select relevant features\n",
    "features = [\n",
    "    'IsHoliday',\n",
    "    'Total_MarkDown',\n",
    "    'Month',\n",
    "    'Fuel_Price',\n",
    "    'Store',\n",
    "    'Week',\n",
    "    'Size',\n",
    "    'mean',\n",
    "    'min',\n",
    "    'CPI',\n",
    "    'Unemployment',\n",
    "    'Dept',\n",
    "    'median',\n",
    "    'max',\n",
    "    'Year',\n",
    "#     'Type'\n",
    "]\n",
    "\n",
    "# preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_data[features])\n",
    "y_train = np.array(train_data['Weekly_Sales'])\n",
    "X_val = scaler.transform(val_data[features])\n",
    "y_val = np.array(val_data['Weekly_Sales'])\n",
    "\n",
    "# create neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# train model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_val, y_val))\n",
    "\n",
    "# evaluate model's performance on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "print('Validation set RMSE:', rmse)\n",
    "\n",
    "# read test dataset\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# preprocess test dataset\n",
    "X_test = scaler.transform(test[features])\n",
    "\n",
    "# make predictions on test dataset\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "# save predictions to file\n",
    "test['Weekly_Sales_Pred'] = y_test\n",
    "test[['id', 'Weekly_Sales_Pred']].to_csv('predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef72c44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 81418056.0000 - val_loss: 19185526.0000\n",
      "Epoch 2/50\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 19757956.0000 - val_loss: 18277698.0000\n",
      "Epoch 3/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 19417822.0000 - val_loss: 18159404.0000\n",
      "Epoch 4/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 19257598.0000 - val_loss: 18040584.0000\n",
      "Epoch 5/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 19200092.0000 - val_loss: 18018018.0000\n",
      "Epoch 6/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 19107726.0000 - val_loss: 17972372.0000\n",
      "Epoch 7/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 19139222.0000 - val_loss: 17896496.0000\n",
      "Epoch 8/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 19096304.0000 - val_loss: 17863800.0000\n",
      "Epoch 9/50\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 19031540.0000 - val_loss: 17746622.0000\n",
      "Epoch 10/50\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 18964266.0000 - val_loss: 17690554.0000\n",
      "Epoch 11/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18918510.0000 - val_loss: 17732452.0000\n",
      "Epoch 12/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18876768.0000 - val_loss: 17619478.0000\n",
      "Epoch 13/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18841526.0000 - val_loss: 17546148.0000\n",
      "Epoch 14/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18785442.0000 - val_loss: 17468040.0000\n",
      "Epoch 15/50\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 18793604.0000 - val_loss: 17497120.0000\n",
      "Epoch 16/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18803822.0000 - val_loss: 17714068.0000\n",
      "Epoch 17/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18779864.0000 - val_loss: 17368846.0000\n",
      "Epoch 18/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18713900.0000 - val_loss: 17348812.0000\n",
      "Epoch 19/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18651008.0000 - val_loss: 17510128.0000\n",
      "Epoch 20/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18686650.0000 - val_loss: 17257402.0000\n",
      "Epoch 21/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18568792.0000 - val_loss: 17212326.0000\n",
      "Epoch 22/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18609702.0000 - val_loss: 17380864.0000\n",
      "Epoch 23/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18607506.0000 - val_loss: 17318542.0000\n",
      "Epoch 24/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18452092.0000 - val_loss: 17142048.0000\n",
      "Epoch 25/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18471314.0000 - val_loss: 17091504.0000\n",
      "Epoch 26/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18521548.0000 - val_loss: 17046410.0000\n",
      "Epoch 27/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18431778.0000 - val_loss: 16960784.0000\n",
      "Epoch 28/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18417842.0000 - val_loss: 17128052.0000\n",
      "Epoch 29/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18346862.0000 - val_loss: 16917446.0000\n",
      "Epoch 30/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18353728.0000 - val_loss: 16741158.0000\n",
      "Epoch 31/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18238290.0000 - val_loss: 16633680.0000\n",
      "Epoch 32/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18142266.0000 - val_loss: 16619323.0000\n",
      "Epoch 33/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 18119430.0000 - val_loss: 16462902.0000\n",
      "Epoch 34/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 17990230.0000 - val_loss: 16354845.0000\n",
      "Epoch 35/50\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 17974896.0000 - val_loss: 16290158.0000\n",
      "Epoch 36/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 17881490.0000 - val_loss: 16228495.0000\n",
      "Epoch 37/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 17852096.0000 - val_loss: 16104108.0000\n",
      "Epoch 38/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 17772976.0000 - val_loss: 16090828.0000\n",
      "Epoch 39/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 17682900.0000 - val_loss: 15908511.0000\n",
      "Epoch 40/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 17601642.0000 - val_loss: 15785723.0000\n",
      "Epoch 41/50\n",
      "819/819 [==============================] - 3s 4ms/step - loss: 17455970.0000 - val_loss: 15678640.0000\n",
      "Epoch 42/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 17410224.0000 - val_loss: 15568778.0000\n",
      "Epoch 43/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 17287764.0000 - val_loss: 15539353.0000\n",
      "Epoch 44/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 17248452.0000 - val_loss: 15397129.0000\n",
      "Epoch 45/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 17034102.0000 - val_loss: 15381576.0000\n",
      "Epoch 46/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 17063952.0000 - val_loss: 15274859.0000\n",
      "Epoch 47/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 17013706.0000 - val_loss: 15059909.0000\n",
      "Epoch 48/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 16939238.0000 - val_loss: 15227548.0000\n",
      "Epoch 49/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 16763182.0000 - val_loss: 15193598.0000\n",
      "Epoch 50/50\n",
      "819/819 [==============================] - 3s 3ms/step - loss: 16765976.0000 - val_loss: 14962823.0000\n",
      "1638/1638 [==============================] - 1s 739us/step\n",
      "Validation set RMSE: 3868.1803952644786\n",
      "3509/3509 [==============================] - 3s 741us/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# read train dataset\n",
    "train = pd.read_csv('train.csv')\n",
    "\n",
    "# split train dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(train, test_size=0.2, random_state=42)\n",
    "\n",
    "# select relevant features\n",
    "# features = ['IsHoliday', 'Total_MarkDown', 'Month', 'Fuel_Price', 'Store', 'Week', 'Size', 'mean', 'min', 'CPI', 'Unemployment', 'Dept', 'median', 'max', 'Year', 'Type']\n",
    "features = [\n",
    "#     'IsHoliday',\n",
    "#     'Total_MarkDown',\n",
    "    'Month',\n",
    "#     'Fuel_Price',\n",
    "    'Store',\n",
    "    'Week',\n",
    "    'Size',\n",
    "    'mean',\n",
    "    'min',\n",
    "#     'CPI',\n",
    "#     'Unemployment',\n",
    "#     'Dept',\n",
    "    'median',\n",
    "    'max',\n",
    "    'std'\n",
    "#     'Year',\n",
    "#     'Type'\n",
    "]\n",
    "# preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_data[features])\n",
    "y_train = np.array(train_data['Weekly_Sales'])\n",
    "X_val = scaler.transform(val_data[features])\n",
    "y_val = np.array(val_data['Weekly_Sales'])\n",
    "\n",
    "# create neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# set up early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# train model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=256, validation_data=(X_val, y_val), callbacks=[early_stop])\n",
    "\n",
    "# evaluate model's performance on validation set\n",
    "y_pred = model.predict(X_val)\n",
    "rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "print('Validation set RMSE:', rmse)\n",
    "\n",
    "# read test dataset\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# preprocess test dataset\n",
    "X_test = scaler.transform(test[features])\n",
    "\n",
    "# make predictions on test dataset\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "# save predictions to file\n",
    "test['Weekly_Sales_Pred'] = y_test\n",
    "test[['id', 'Weekly_Sales_Pred']].to_csv('predictions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae666a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae539b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083a16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c4285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2394b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8260159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cd39ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb70efb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df605d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a9b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d5f043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b38987b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b239e9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aee2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc31d4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc959e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
